{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ebd44e2b-6ae8-4a00-a5e2-9357a82997f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install langchain\n",
    "# !pip install transformers\n",
    "# !pip install faiss-cpu\n",
    "# !pip install chromadb\n",
    "# !pip install rouge-score\n",
    "# !pip install streamlit fastapi\n",
    "# ! pip install -U langchain-community\n",
    "# !pip install sentence-transformers\n",
    "# !pip install openai==0.28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "db8d5530-60e1-41f6-8007-2826da16856a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "da1bf36a-2b65-44f2-8e38-1e2c2e9f16a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import requests\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from bs4 import BeautifulSoup\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.llms.base import LLM\n",
    "from rouge_score import rouge_scorer\n",
    "from langchain.cache import InMemoryCache\n",
    "import streamlit as st\n",
    "import openai\n",
    "from typing import Optional, List"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faadfb24-6776-4716-8cc3-414be6cdf81d",
   "metadata": {},
   "source": [
    "# Corpus Preparation\n",
    "- **Corpus Preparation**: We obtained a domain-specific corpus from a sample URL (a quantum computing blog) and preprocessed it by cleaning the HTML tags.\n",
    "- **Text Splitting**: The text was split into smaller chunks using Langchain’s `RecursiveCharacterTextSplitter` with a chunk size of 500 characters and a 100-character overlap to ensure context continuity between chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "da31890d-adb1-4145-beca-079db80fd1ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text cleaning\n",
    "def fetch_text_from_url(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    # Remove unwanted HTML tags\n",
    "    for tag in soup([\"script\", \"style\"]):\n",
    "        tag.decompose()\n",
    "\n",
    "    # Get text content\n",
    "    text = ' '.join([p.get_text() for p in soup.find_all('p')])\n",
    "    return text\n",
    "\n",
    "# URL input\n",
    "url = \"https://medium.com/@vignesh2659/quantum-computing-abd85aa5da9d\"\n",
    "corpus = fetch_text_from_url(url)\n",
    "\n",
    "# Split the text into smaller chunks using Langchain's TextSplitter\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)\n",
    "corpus_chunks = splitter.split_text(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "85e86dcf-e650-4023-a98b-31d7e39bff75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Sign up Sign in Sign up Sign in Vignesh R Follow -- Listen Share Quantum Realm in the MARVEL multiverse, our very own Ant-man has one of the coolest superpowers out there. He becomes either too tiny or too huge. This fancy term is indeed nice to talk about. Introduction Likewise, back to our topic of interest, there comes Quantum computing. Quantum computers are powerful than supercomputers and are present in Google, IBM, and Rigetti. A fancy term here is Quantum Supremacy. Google had achieved',\n",
       " 'present in Google, IBM, and Rigetti. A fancy term here is Quantum Supremacy. Google had achieved Quantum Supremacy with its Quantum computer in 2019, Sycamore. It can perform a calculation in mere seconds which might take the world’s fastest supercomputer around thousands of years. That is the level the world has gone into. Sycamore is a 53-qubit computer. Such computers need to be kept under conditions with a temperature close to absolute zero. Quantum Physics Quantum computing falls under a']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Displaying a sample chunk\n",
    "corpus_chunks[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d4b4849-f51e-42b7-b376-176446836f8e",
   "metadata": {},
   "source": [
    "# Vectorize the Corpus\n",
    "- **Embedding Generation**: We used `HuggingFaceEmbeddings` to create embeddings from the pre-trained model `sentence-transformers/all-mpnet-base-v2`. This allows us to convert each chunk of the corpus into a numerical vector that captures its semantic meaning.\n",
    "- **Vector Store**: The embeddings were stored in a FAISS vector store, which is an efficient structure for storing and retrieving vectors based on similarity search.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "31d7d9b0-819f-43a6-83c2-5dfc221f2269",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\r.vignesh\\AppData\\Local\\Temp\\ipykernel_8836\\1689185818.py:2: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")\n",
      "C:\\Users\\r.vignesh\\Anaconda3\\envs\\rag_env_bytesized\\lib\\site-packages\\sentence_transformers\\cross_encoder\\CrossEncoder.py:13: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    }
   ],
   "source": [
    "# Initialize HuggingFace embeddings\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")\n",
    "\n",
    "# Create embeddings for the chunks\n",
    "chunk_embeddings = embedding_model.embed_documents(corpus_chunks)\n",
    "\n",
    "# Initialize FAISS vector store and add texts with their embeddings\n",
    "vector_store = FAISS.from_texts(texts=corpus_chunks, embedding=embedding_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa890163-bbec-4f3b-9c34-fae4e74bb4ab",
   "metadata": {},
   "source": [
    "# Implement the Retrieval Component\n",
    "- **Retriever Setup**: We initialized a FAISS-based retriever from the stored embeddings. The retriever efficiently finds the top relevant documents for any input query by comparing the similarity between the query and document vectors.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a6070252-2b26-4f12-afdc-5fe6cfc6228e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 1: with a temperature close to absolute zero. Quantum Physics Quantum computing falls under a study called Quantum Physics. Quantum computing’s heart and soul resides in what we call as Qubits (Quantum b...\n",
      "\n",
      "Document 2: either a head or a tail. However, during the spin, there are 2 possibilities, both head and a tail. That position of a coin having both at the same time is what a qubit is. This state is called Superp...\n",
      "\n",
      "Document 3: answers cancel each other out. Hence this way, amplitude with the right answer remains as the only possible outcome. Quantum computers function using a process called superconductivity. We have a chip...\n",
      "\n",
      "Document 4: technical aspect of it. In Quantum Mechanics, the following is as explained by Scott Aaronson, who is a Quantum scientist from the University of Texas in Austin. Amplitude, an amplitude of a positive ...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\r.vignesh\\AppData\\Local\\Temp\\ipykernel_8836\\718014876.py:6: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  retrieved_docs = retriever.get_relevant_documents(query)\n"
     ]
    }
   ],
   "source": [
    "# Retriever using FAISS\n",
    "retriever = vector_store.as_retriever()\n",
    "\n",
    "# Example\n",
    "query = \"What is Superposition?\"\n",
    "retrieved_docs = retriever.get_relevant_documents(query)\n",
    "\n",
    "# Display retrieved results\n",
    "for i, doc in enumerate(retrieved_docs):\n",
    "    print(f\"Document {i + 1}: {doc.page_content[:200]}...\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8453f47-a5bf-4041-9c6e-9629843688f7",
   "metadata": {},
   "source": [
    "# Build the Generation Component\n",
    "- **Generation with GPT-4o-mini**: We created a function to combine the retrieved document chunks and pass them as context to OpenAI's GPT-4o-mini model, which generates a coherent response based on the query and context.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6547adea-be5c-41ba-b020-1c71f998dd36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a response using the GPT-4o-mini model\n",
    "def generate_response(query, retrieved_docs): \n",
    "    context = \" \".join([doc.page_content for doc in retrieved_docs]) # Combine into a single context\n",
    "    \n",
    "    # Using OpenAI's API\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=\"gpt-4o-mini\",  \n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": f\"Context: {context}\"},\n",
    "            {\"role\": \"user\", \"content\": query}\n",
    "        ],\n",
    "        max_tokens=300,\n",
    "        temperature=0.7\n",
    "    )\n",
    "\n",
    "    # Extracting the first choice's content from the response\n",
    "    return response['choices'][0]['message']['content']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ebe21b8-4a98-4c66-bf0e-f5bf65d315db",
   "metadata": {},
   "source": [
    "# Combine the Retrieval and Generation Components\n",
    "- **Retrieval-Augmented Generation**: We used Langchain’s `RetrievalQA` to combine the retrieval and generation steps into one integrated system. This ensures that the language model generates responses based on the most relevant retrieved documents.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3cd7d06b-d4eb-474f-a8c9-4291f0248456",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\r.vignesh\\AppData\\Local\\Temp\\ipykernel_8836\\2561254141.py:22: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  response_rag = rag_model.run(query)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAG Response: Superposition is a fundamental concept in quantum computing, as described in the context provided. It allows qubits, the building blocks of quantum computers, to exist in multiple states simultaneously. Unlike classical bits, which can only be in one of two states (0 or 1), qubits can represent both 0 and 1 at the same time due to superposition. This ability to be in multiple states enhances the computational power of quantum computers, enabling them to process a vast amount of information simultaneously.\n",
      "\n",
      "Superposition is crucial for the functioning of quantum algorithms, such as those utilizing the Grover Operator, which systematically eliminates incorrect possibilities and amplifies the probability of the correct answer. By leveraging superposition, quantum computers can perform complex calculations more efficiently than classical computers, making it a key aspect of their potential to solve problems that are currently intractable. Overall, superposition plays a vital role in the enhanced performance and capabilities of quantum computing.\n"
     ]
    }
   ],
   "source": [
    "# Custom wrapper for Openai function\n",
    "class GptLLM(LLM):\n",
    "    def _call(self, prompt: str, retrieved_docs: List = [], stop: Optional[List[str]] = None) -> str:\n",
    "        # Initialize the LLM (gpt-4o-mini) with your OpenAI API key\n",
    "        openai.api_key = \"key\"\n",
    "        return generate_response(prompt, retrieved_docs)\n",
    "\n",
    "    @property\n",
    "    def _identifying_params(self) -> dict:\n",
    "        return {\"model\": \"gpt-4o-mini\"}\n",
    "\n",
    "    @property\n",
    "    def _llm_type(self) -> str:\n",
    "        return \"custom\"\n",
    "\n",
    "# Initializing the Gpt LLM and the RAG model\n",
    "openaillm = GptLLM()\n",
    "rag_model = RetrievalQA.from_chain_type(llm=openaillm, retriever=retriever)\n",
    "\n",
    "# Example\n",
    "query = \"Explain the importance of Superposition from the context provided.\"\n",
    "response_rag = rag_model.run(query)\n",
    "print(f\"RAG Response: {response_rag}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9b98c29-966e-4e09-a1b6-f276d5ff57e4",
   "metadata": {},
   "source": [
    "# Evaluation and Iteration\n",
    "- **ROUGE Evaluation**: We used ROUGE scores to evaluate how well the generated answers from both the GPT-4o-mini model and the RAG model align with the reference answer. ROUGE is a common metric to measure the overlap between generated and reference text in natural language processing tasks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "01666bde-ac79-4799-bc10-b891599b1361",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROUGE Scores RAG: {'rouge1': Score(precision=0.14193548387096774, recall=0.6111111111111112, fmeasure=0.23036649214659685), 'rougeL': Score(precision=0.07741935483870968, recall=0.3333333333333333, fmeasure=0.1256544502617801)}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Calculating ROUGE scores for evaluation\n",
    "def evaluate_rouge(prediction, reference):\n",
    "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rougeL'], use_stemmer=True)\n",
    "    scores = scorer.score(reference, prediction)\n",
    "    return scores\n",
    "\n",
    "# Example\n",
    "reference_answer = \"During the spin of a coin, there are 2 possibilities, both head and a tail. That position of a coin having both at the same time is what a qubit is. This state is called Superposition.\"\n",
    "\n",
    "# Evaluating RAG-generated answer using ROUGE\n",
    "rouge_scores_rag = evaluate_rouge(response_rag, reference_answer)\n",
    "print(\"ROUGE Scores RAG:\", rouge_scores_rag)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bc3b666-c132-450f-bd5c-db6f4b6db475",
   "metadata": {},
   "source": [
    "# Advanced Topics - Streamlit run\n",
    "- **Caching**: We implemented a Python dictionary-based caching system to store the results of previously processed queries, improving performance for repeated queries.\n",
    "- **Streamlit Interface**: We created a Streamlit-based interface to allow users to interact with the RAG model in real-time by inputting their queries and seeing the model's responses.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b31d087c-af15-4eec-ad3d-59a0c2ebc784",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the RAG model using Langchain's RetrievalQA with custom LLM\n",
    "rag_model = RetrievalQA.from_chain_type(llm=openaillm, retriever=retriever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "af60898b-6607-4710-b3c0-b32a8cf745a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-17 00:59:41.985 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-10-17 00:59:41.986 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-10-17 00:59:41.988 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-10-17 00:59:41.989 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-10-17 00:59:41.989 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-10-17 00:59:41.990 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-10-17 00:59:41.991 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-10-17 00:59:41.992 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n"
     ]
    }
   ],
   "source": [
    "# Streamlit-based interface for querying the RAG model\n",
    "st.title(\"RAG Model Interface\")\n",
    "\n",
    "query = st.text_input(\"Enter your question:\")\n",
    "if query:\n",
    "    response_rag = rag_model.run(query)\n",
    "    st.write(f\"Response: {response_rag}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
