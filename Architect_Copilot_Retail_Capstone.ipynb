{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e7c7ec7c",
   "metadata": {},
   "source": [
    "# The Architect's Co-Pilot for Adaptive Retail Design with GCP and Gemini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd0f1a0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "langchain-google-genai 2.1.10 requires google-ai-generativelanguage<0.7.0,>=0.6.18, but you have google-ai-generativelanguage 0.6.15 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# !python -m pip install --quiet python-dotenv google-generativeai pinecone pypdf tiktoken langgraph pydantic pytrends mlflow matplotlib langchain-pinecone\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6bf03abc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os, json, time, math, hashlib, uuid\n",
    "from typing import List, Dict, Any, Optional\n",
    "from dotenv import load_dotenv; load_dotenv()\n",
    "\n",
    "import google.generativeai as genai\n",
    "from pinecone import Pinecone, ServerlessSpec\n",
    "from pypdf import PdfReader\n",
    "import mlflow\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Rectangle\n",
    "\n",
    "from pydantic import BaseModel\n",
    "from langgraph.graph import StateGraph, END\n",
    "from pytrends.request import TrendReq\n",
    "\n",
    "required_env = [\"GOOGLE_API_KEY\", \"GOOGLE_GENAI_USE_VERTEXAI\", \"GOOGLE_CLOUD_LOCATION\", \"GOOGLE_CLOUD_PROJECT\", \"PINECONE_API_KEY\"]\n",
    "missing = [k for k in required_env if not os.getenv(k)]\n",
    "if missing: raise EnvironmentError(f\"Missing env vars: {missing}\")\n",
    "assert os.getenv(\"GOOGLE_GENAI_USE_VERTEXAI\") == \"False\"\n",
    "\n",
    "genai.configure(api_key=os.getenv(\"GOOGLE_API_KEY\"))\n",
    "EMBED_MODEL = \"text-embedding-004\"\n",
    "GEN_MODEL   = \"gemini-2.0-flash\"\n",
    "\n",
    "PINECONE_INDEX_NAME = \"blue-retail-docs\"\n",
    "PINECONE_CLOUD = \"aws\"; PINECONE_REGION = \"us-east-1\"; VECTOR_DIM = 768\n",
    "\n",
    "# DOCS_DIR = \"./docs\"; OUTPUTS_DIR = \"./outputs\"\n",
    "# os.makedirs(DOCS_DIR, exist_ok=True); os.makedirs(OUTPUTS_DIR, exist_ok=True)\n",
    "# print(\"Ready with:\", EMBED_MODEL, GEN_MODEL, PINECONE_INDEX_NAME)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "965315aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def sha256_text(s: str) -> str:\n",
    "    import hashlib; return hashlib.sha256(s.encode(\"utf-8\")).hexdigest()\n",
    "\n",
    "def read_pdf_text(path: str) -> List[Dict[str, Any]]:\n",
    "    pages = []\n",
    "    with open(path, \"rb\") as f:\n",
    "        pdf = PdfReader(f)\n",
    "        for i, page in enumerate(pdf.pages, start=1):\n",
    "            try: txt = page.extract_text() or \"\"\n",
    "            except Exception: txt = \"\"\n",
    "            pages.append({\"page\": i, \"text\": txt})\n",
    "    return pages\n",
    "\n",
    "def read_txt(path: str) -> str:\n",
    "    with open(path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f: return f.read()\n",
    "\n",
    "def recursive_chunk(text: str, max_chars: int = 2000, overlap: int = 200) -> List[str]:\n",
    "    chunks=[]; i=0; n=len(text)\n",
    "    while i<n:\n",
    "        end=min(i+max_chars,n); chunk=text[i:end]; chunks.append(chunk.strip()); i=end-overlap\n",
    "        if i<0: i=0\n",
    "        if i>=n: break\n",
    "    return [c for c in chunks if c]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9955068d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Pinecone index...\n",
      "Index ready: blue-retail-docs\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def embed_texts(texts: List[str], model: str = EMBED_MODEL) -> List[List[float]]:\n",
    "    out=[]\n",
    "    for t in texts:\n",
    "        resp = genai.embed_content(model=model, content=t)\n",
    "        out.append(resp[\"embedding\"])\n",
    "    return out\n",
    "\n",
    "pc = Pinecone(api_key=os.getenv(\"PINECONE_API_KEY\"))\n",
    "existing = [idx[\"name\"] for idx in pc.list_indexes()]\n",
    "if PINECONE_INDEX_NAME not in existing:\n",
    "    print(\"Creating Pinecone index...\")\n",
    "    pc.create_index(name=PINECONE_INDEX_NAME, dimension=VECTOR_DIM, metric=\"cosine\",\n",
    "                    spec=ServerlessSpec(cloud=PINECONE_CLOUD, region=PINECONE_REGION))\n",
    "    while True:\n",
    "        if pc.describe_index(PINECONE_INDEX_NAME).status[\"ready\"]: break\n",
    "        time.sleep(2)\n",
    "index = pc.Index(PINECONE_INDEX_NAME)\n",
    "print(\"Index ready:\", PINECONE_INDEX_NAME)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "195ecb54",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "DOC_PATHS = [\n",
    "    # \"./docs/Blue_Retail_Brand_Book_v4.pdf\",\n",
    "    # \"./docs/Fixture_Catalog_Q3_2025.pdf\",\n",
    "    # \"./docs/National_Building_Code_Accessibility_Chapter.txt\",\n",
    "    # \"./docs/Store_Leasing_Agreement_Surat.pdf\",\n",
    "    # \"./docs/Retail_Design_Best_Practices.md\",\n",
    "]\n",
    "\n",
    "def detect_doctype(path: str) -> str:\n",
    "    name=os.path.basename(path).lower()\n",
    "    if \"brand\" in name: return \"brand\"\n",
    "    if \"fixture\" in name: return \"fixture\"\n",
    "    if \"leasing\" in name or \"lease\" in name: return \"lease\"\n",
    "    if \"accessibility\" in name or \"nbc\" in name: return \"nbc\"\n",
    "    if name.endswith(\".md\"): return \"best_practices\"\n",
    "    if name.endswith(\".txt\"): return \"text\"\n",
    "    if name.endswith(\".pdf\"): return \"pdf\"\n",
    "    return \"other\"\n",
    "\n",
    "def ingest_documents(paths: List[str]):\n",
    "    items=[]\n",
    "    for p in paths:\n",
    "        if not os.path.exists(p):\n",
    "            print(\"[WARN] Missing:\", p); continue\n",
    "        doctype=detect_doctype(p); base=os.path.basename(p)\n",
    "        if p.lower().endswith(\".pdf\"):\n",
    "            for page_obj in read_pdf_text(p):\n",
    "                page=page_obj[\"page\"]; text=page_obj[\"text\"]\n",
    "                if not text.strip(): continue\n",
    "                for i,ch in enumerate(recursive_chunk(text,1800,200)):\n",
    "                    cid=f\"{base}-{page}-{i}-{uuid.uuid4().hex[:8]}\"\n",
    "                    items.append((cid,ch,{\"source_document\":base,\"doctype\":doctype,\"page\":page}))\n",
    "        elif p.lower().endswith(\".txt\") or p.lower().endswith(\".md\"):\n",
    "            content=read_txt(p); \n",
    "            for i,ch in enumerate(recursive_chunk(content,1800,200)):\n",
    "                cid=f\"{base}-{i}-{uuid.uuid4().hex[:8]}\"\n",
    "                items.append((cid,ch,{\"source_document\":base,\"doctype\":doctype}))\n",
    "    if not items:\n",
    "        print(\"No content collected. Add files to DOC_PATHS and re-run.\"); return\n",
    "    texts=[t for _,t,_ in items]; embeds=embed_texts(texts)\n",
    "    vecs=[]\n",
    "    for (cid,text,meta),vec in zip(items,embeds):\n",
    "        m=dict(meta); m.update({\"hash\":sha256_text(text),\"len\":len(text)})\n",
    "        vecs.append({\"id\":cid,\"values\":vec,\"metadata\":m})\n",
    "    for i in range(0,len(vecs),100):\n",
    "        index.upsert(vectors=vecs[i:i+100])\n",
    "    print(\"Upserted\", len(vecs), \"chunks.\")\n",
    "\n",
    "ingest_documents(DOC_PATHS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f460b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def pinecone_search(query: str, top_k: int = 8, filter_meta: Optional[Dict[str, Any]] = None):\n",
    "    qv = embed_texts([query])[0]\n",
    "    return index.query(vector=qv, top_k=top_k, include_metadata=True, filter=filter_meta or {})\n",
    "print(\"Smoke query:\", [m[\"id\"] for m in pinecone_search(\"emergency exit\",3).get(\"matches\",[])])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb883f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_city_trend_weights(city: str, categories: List[str]) -> Dict[str, float]:\n",
    "    pytrends = TrendReq(hl=\"en-US\", tz=330)\n",
    "    try:\n",
    "        pytrends.build_payload(kw_list=categories, timeframe='now 7-d', geo='IN')\n",
    "        df = pytrends.interest_over_time()\n",
    "        if df is None or df.empty: raise ValueError(\"Empty trends\")\n",
    "        latest = df.iloc[-1][categories].to_dict()\n",
    "    except Exception:\n",
    "        latest = {k: 50 for k in categories}\n",
    "    mx = max(latest.values()) if latest else 1.0\n",
    "    if mx == 0: mx = 1.0\n",
    "    return {k: round(v/mx,2) for k,v in latest.items()}\n",
    "\n",
    "DEFAULT_CATEGORIES = [\"mobiles\",\"laptops\",\"audio\",\"gaming\",\"appliances\"]\n",
    "get_city_trend_weights(\"Surat\", DEFAULT_CATEGORIES)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5f37dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Pydantic schema\n",
    "class EntryPoint(BaseModel):\n",
    "    id: str; x: float; y: float; width_m: float\n",
    "class ZoneRect(BaseModel):\n",
    "    x: float; y: float; w: float; h: float\n",
    "class Zone(BaseModel):\n",
    "    zone_id: str; name: str; priority: float; area_sqm: float; rect: ZoneRect\n",
    "class Fixture(BaseModel):\n",
    "    fixture_id: str; zone_id: str; x: float; y: float; w: float; h: float; rotation_deg: float = 0\n",
    "class ComplianceReport(BaseModel):\n",
    "    brand_rules_ok: bool; lease_ok: bool; nbc_ok: bool; violations: List[Dict[str, Any]]\n",
    "class LayoutPlan(BaseModel):\n",
    "    site: Dict[str, Any]\n",
    "    zoning: List[Zone]\n",
    "    fixtures: List[Fixture]\n",
    "    paths: Dict[str, Any]\n",
    "    constraints: Dict[str, Any]\n",
    "    citations: List[Dict[str, Any]]\n",
    "    compliance_report: Optional[ComplianceReport] = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f16353d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def build_context_from_matches(matches: List[Dict[str, Any]], max_lines: int = 20) -> str:\n",
    "    lines=[]\n",
    "    for m in matches[:max_lines]:\n",
    "        md=m.get(\"metadata\",{}); src=md.get(\"source_document\",\"unknown\"); page=md.get(\"page\",\"?\")\n",
    "        lines.append(f\"[{m['id']}] {src} p.{page}\")\n",
    "    return \"\\n\".join(lines)\n",
    "\n",
    "def strategist_generate_layout(city: str, floor_area_sqm: float, entry_points: List[Dict[str, Any]], trend_weights: Dict[str,float],\n",
    "                               top_k:int=12, corridor_min_m: float=1.2) -> LayoutPlan:\n",
    "    matches = pinecone_search(f\"layout constraints for {city}; NBC; lease; brand rules; fixtures\", top_k=top_k).get(\"matches\",[])\n",
    "    citations=[{\"chunk_id\":m.get(\"id\"),\"source_document\":m.get(\"metadata\",{}).get(\"source_document\"),\"page\":m.get(\"metadata\",{}).get(\"page\")} for m in matches]\n",
    "    retrieval_context = build_context_from_matches(matches)\n",
    "    user_prompt = (\n",
    "        \"You are the Layout Strategist for a premium electronics retailer. \"\n",
    "        \"Synthesize a compliant conceptual layout only from retrieved documents and trend weights. \"\n",
    "        \"Return STRICT JSON matching the schema (zones, fixtures, paths with min corridor width, constraints, citations).\\n\\n\"\n",
    "        f\"City: {city}\\nFloor area (sqm): {floor_area_sqm}\\nEntry points: {entry_points}\\n\"\n",
    "        f\"Trend weights (0..1): {trend_weights}\\nMin corridor width: {corridor_min_m} m\\n\"\n",
    "        \"Include 'citations' that reference the chunk ids.\"\n",
    "    )\n",
    "    model = genai.GenerativeModel(GEN_MODEL)\n",
    "    last_err=None\n",
    "    for _ in range(3):\n",
    "        try:\n",
    "            g = model.generate_content([retrieval_context, user_prompt])\n",
    "            data = json.loads(g.text)\n",
    "            plan = LayoutPlan(**data)\n",
    "            if not plan.citations: plan.citations = citations\n",
    "            return plan\n",
    "        except Exception as e:\n",
    "            last_err=e; time.sleep(1)\n",
    "    raise RuntimeError(f\"Failed to produce valid layout JSON: {last_err}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d79a475",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def compliance_check(plan: LayoutPlan) -> ComplianceReport:\n",
    "    min_w = plan.paths.get(\"min_corridor_width_m\", 1.2)\n",
    "    violations=[]\n",
    "    for c in plan.paths.get(\"corridors\", []):\n",
    "        width=min(c[\"w\"], c[\"h\"])\n",
    "        if width < min_w:\n",
    "            violations.append({\"type\":\"corridor_width\",\"required_m\":float(min_w),\"actual_m\":float(width),\"location\":c})\n",
    "    ok = len(violations)==0\n",
    "    return ComplianceReport(brand_rules_ok=True, lease_ok=True, nbc_ok=ok, violations=violations)\n",
    "\n",
    "def render_layout_png(plan: LayoutPlan, out_path: str):\n",
    "    site=plan.site; dims=site.get(\"dimensions_m\") or {\"width\": math.sqrt(site.get(\"floor_area_sqm\", 200)), \"height\": math.sqrt(site.get(\"floor_area_sqm\", 200))/1.5}\n",
    "    W=dims[\"width\"]; H=dims[\"height\"]\n",
    "    fig,ax=plt.subplots(figsize=(10,6)); ax.add_patch(Rectangle((0,0),W,H,fill=False,linewidth=2))\n",
    "    for e in site.get(\"entry_points\", []):\n",
    "        ax.add_patch(Rectangle((e[\"x\"], e[\"y\"]-0.1),0.2,0.2,fill=True)); ax.text(e[\"x\"]+0.25,e[\"y\"],f\"Entry {e['id']}\",va='center')\n",
    "    for z in plan.zoning:\n",
    "        r=z.rect; ax.add_patch(Rectangle((r.x,r.y),r.w,r.h,fill=False)); ax.text(r.x+r.w/2,r.y+r.h/2,z.name,ha='center',va='center',fontsize=9)\n",
    "    for fx in plan.fixtures:\n",
    "        ax.add_patch(Rectangle((fx.x,fx.y),fx.w,fx.h,fill=True)); ax.text(fx.x+fx.w/2,fx.y+fx.h/2,fx.fixture_id,ha='center',va='center',fontsize=6)\n",
    "    for c in plan.paths.get(\"corridors\", []):\n",
    "        ax.add_patch(Rectangle((c[\"x\"],c[\"y\"]),c[\"w\"],c[\"h\"],fill=False,linestyle='--')); ax.text(c[\"x\"]+c[\"w\"]/2,c[\"y\"]+c[\"h\"]/2,'Corridor',ha='center',va='center',fontsize=7)\n",
    "    ax.set_xlim(0,W); ax.set_ylim(0,H); ax.set_aspect('equal', adjustable='box'); ax.set_title(f\"Conceptual Layout – {site.get('city','')}\")\n",
    "    ax.set_xlabel(\"Meters (X)\"); ax.set_ylabel(\"Meters (Y)\"); plt.tight_layout(); plt.savefig(out_path,dpi=150); plt.close(fig)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fcf028d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def log_run_to_mlflow(city: str, trend_weights: Dict[str,float], plan: LayoutPlan, png_path: str, index_name: str):\n",
    "    mlflow.set_experiment(\"architect_copilot\")\n",
    "    with mlflow.start_run(run_name=f\"layout_{city}\"):\n",
    "        mlflow.log_param(\"city\", city); mlflow.log_param(\"index\", index_name)\n",
    "        mlflow.log_param(\"embed_model\", EMBED_MODEL); mlflow.log_param(\"gen_model\", GEN_MODEL)\n",
    "        mlflow.log_param(\"corridor_min_m\", plan.paths.get(\"min_corridor_width_m\", 1.2))\n",
    "        for k,v in trend_weights.items(): mlflow.log_param(f\"trend_{k}\", v)\n",
    "        comp = plan.compliance_report.model_dump() if plan.compliance_report else {}\n",
    "        mlflow.log_metric(\"nbc_ok\", int(comp.get(\"nbc_ok\", 0))); mlflow.log_metric(\"violations_count\", len(comp.get(\"violations\", [])) if comp else 0)\n",
    "        json_path=os.path.join(\"./outputs\", f\"layout_plan_{city}.json\")\n",
    "        with open(json_path,\"w\") as f: json.dump(plan.model_dump(), f, indent=2)\n",
    "        mlflow.log_artifact(json_path); \n",
    "        if os.path.exists(png_path): mlflow.log_artifact(png_path)\n",
    "        rid = mlflow.active_run().info.run_id\n",
    "    return rid\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be621b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "CITY=\"Surat\"; FLOOR_AREA_SQM=240.0; ENTRY_POINTS=[{\"id\":\"E1\",\"x\":0.0,\"y\":6.0,\"width_m\":2.4}]; CATEGORIES=DEFAULT_CATEGORIES; CORRIDOR_MIN=1.2\n",
    "trend_weights = get_city_trend_weights(CITY, CATEGORIES); print(\"Trend weights:\", trend_weights)\n",
    "try:\n",
    "    plan = strategist_generate_layout(CITY, FLOOR_AREA_SQM, ENTRY_POINTS, trend_weights, top_k=10, corridor_min_m=CORRIDOR_MIN)\n",
    "except RuntimeError as e:\n",
    "    print(\"[WARN]\", e, \"— using baseline sample.\")\n",
    "    plan = LayoutPlan(\n",
    "        site={\"city\": CITY, \"floor_area_sqm\": FLOOR_AREA_SQM, \"dimensions_m\":{\"width\":20.0,\"height\":12.0}, \"entry_points\": ENTRY_POINTS},\n",
    "        zoning=[\n",
    "            {\"zone_id\":\"Z1\",\"name\":\"Mobiles\",\"priority\":0.92,\"area_sqm\":60.0,\"rect\":{\"x\":1.0,\"y\":6.5,\"w\":9.0,\"h\":4.5}},\n",
    "            {\"zone_id\":\"Z2\",\"name\":\"Laptops\",\"priority\":0.81,\"area_sqm\":50.0,\"rect\":{\"x\":1.0,\"y\":1.0,\"w\":9.0,\"h\":4.5}},\n",
    "            {\"zone_id\":\"Z3\",\"name\":\"Audio\",\"priority\":0.68,\"area_sqm\":35.0,\"rect\":{\"x\":11.0,\"y\":1.0,\"w\":8.0,\"h\":5.0}},\n",
    "            {\"zone_id\":\"Z4\",\"name\":\"Gaming\",\"priority\":0.55,\"area_sqm\":30.0,\"rect\":{\"x\":11.0,\"y\":7.0,\"w\":8.0,\"h\":4.5}}\n",
    "        ],\n",
    "        fixtures=[\n",
    "            {\"fixture_id\":\"FX-Table-120x80\",\"zone_id\":\"Z1\",\"x\":2.0,\"y\":7.0,\"w\":1.2,\"h\":0.8},\n",
    "            {\"fixture_id\":\"FX-Table-120x80\",\"zone_id\":\"Z1\",\"x\":4.0,\"y\":7.0,\"w\":1.2,\"h\":0.8},\n",
    "            {\"fixture_id\":\"FX-Table-120x80\",\"zone_id\":\"Z2\",\"x\":2.0,\"y\":2.0,\"w\":1.2,\"h\":0.8},\n",
    "            {\"fixture_id\":\"FX-Table-120x80\",\"zone_id\":\"Z2\",\"x\":4.0,\"y\":2.0,\"w\":1.2,\"h\":0.8},\n",
    "            {\"fixture_id\":\"FX-Wall-Display\",\"zone_id\":\"Z3\",\"x\":11.0,\"y\":1.0,\"w\":0.5,\"h\":5.0},\n",
    "            {\"fixture_id\":\"FX-Demo-Console\",\"zone_id\":\"Z4\",\"x\":12.0,\"y\":8.0,\"w\":2.0,\"h\":1.0}\n",
    "        ],\n",
    "        paths={\"min_corridor_width_m\": CORRIDOR_MIN, \"corridors\":[{\"x\":10.2,\"y\":1.0,\"w\":0.8,\"h\":10.0},{\"x\":1.0,\"y\":5.7,\"w\":18.0,\"h\":0.8}]},\n",
    "        constraints={\"brand_rules\": [\"logo_sightline_from_entry\"], \"lease\":[\"emergency_exit_clear_1.5m\"], \"nbc_accessibility\":[\"ramp_slope_1_in_12\"]},\n",
    "        citations=[]\n",
    "    )\n",
    "plan.compliance_report = compliance_check(plan)\n",
    "png_path=os.path.join(\"./outputs\", f\"layout_{CITY}.png\"); render_layout_png(plan, png_path)\n",
    "json_path=os.path.join(\"./outputs\", f\"layout_plan_{CITY}.json\"); \n",
    "with open(json_path,\"w\") as f: json.dump(plan.model_dump(), f, indent=2)\n",
    "print(\"Saved:\", json_path, \"|\", png_path); print(\"Compliance:\", plan.compliance_report.model_dump() if plan.compliance_report else None)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9af921b2",
   "metadata": {},
   "source": [
    "## (Deployment – commented for later)\n",
    "Uncomment and adapt when deploying Streamlit/FastAPI to Cloud Run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8caca313",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# %%bash\n",
    "# cat > Dockerfile <<'EOF'\n",
    "# FROM python:3.11-slim\n",
    "# WORKDIR /app\n",
    "# COPY requirements.txt .\n",
    "# RUN pip install -r requirements.txt\n",
    "# COPY . .\n",
    "# ENV PORT=8080\n",
    "# CMD [\"streamlit\", \"run\", \"ui/app.py\", \"--server.port=8080\", \"--server.address=0.0.0.0\"]\n",
    "# EOF\n",
    "# echo \"Dockerfile scaffold written.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa807671",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # gcloud builds submit --tag gcr.io/${GOOGLE_CLOUD_PROJECT}/architect-copilot:latest\n",
    "# # gcloud run deploy architect-copilot --image gcr.io/${GOOGLE_CLOUD_PROJECT}/architect-copilot:latest --platform=managed --region=us-central1 # #   --set-env-vars GOOGLE_CLOUD_PROJECT=${GOOGLE_CLOUD_PROJECT},GOOGLE_CLOUD_LOCATION=global,GOOGLE_GENAI_USE_VERTEXAI=False # #   --set-secrets GOOGLE_API_KEY=GOOGLE_API_KEY:latest,PINECONE_API_KEY=PINECONE_API_KEY:latest\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
